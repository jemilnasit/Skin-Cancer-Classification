{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXowFCc3mU4v",
        "outputId": "9c406e30-7cd8-487a-bb3a-0859bc4f4872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Drfh8-nmb1q"
      },
      "outputs": [],
      "source": [
        "!tar -xf /content/drive/MyDrive/skin_cancer/final_data.tar -C /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FESU8uomd1P",
        "outputId": "a68202b0-cbf8-4d8a-a300-f36354928b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 27205 images belonging to 7 classes.\n",
            "Found 235 images belonging to 7 classes.\n",
            "Found 1470 images belonging to 7 classes.\n",
            "Counts per class: [3000 3500 4000 3000 4000 6705 3000]\n",
            "New alpha_array: [0.15805851 0.14633375 0.13688269 0.15805851 0.13688269 0.10572535\n",
            " 0.15805851]\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_tiny_notop.h5\n",
            "\u001b[1m111650432/111650432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640ms/step - accuracy: 0.2560 - auc: 0.6311 - loss: 0.3795 - precision: 0.2999 - recall: 0.1520\n",
            "Epoch 1: val_loss improved from inf to 0.23630, saving model to /content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints/ckpt-01.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 658ms/step - accuracy: 0.2560 - auc: 0.6312 - loss: 0.3794 - precision: 0.2999 - recall: 0.1521 - val_accuracy: 0.5745 - val_auc: 0.8617 - val_loss: 0.2363 - val_precision: 0.7874 - val_recall: 0.4255 - learning_rate: 1.0000e-04\n",
            "Epoch 2/2\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619ms/step - accuracy: 0.3927 - auc: 0.7716 - loss: 0.2911 - precision: 0.4958 - recall: 0.2595\n",
            "Epoch 2: val_loss improved from 0.23630 to 0.22302, saving model to /content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints/ckpt-02.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 623ms/step - accuracy: 0.3927 - auc: 0.7716 - loss: 0.2911 - precision: 0.4959 - recall: 0.2595 - val_accuracy: 0.6213 - val_auc: 0.8791 - val_loss: 0.2230 - val_precision: 0.8060 - val_recall: 0.4596 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942ms/step - accuracy: 0.4921 - auc: 0.8463 - loss: 0.2478 - precision: 0.6061 - recall: 0.3572\n",
            "Epoch 3: val_loss improved from 0.22302 to 0.17415, saving model to /content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints/ckpt-03.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m856s\u001b[0m 960ms/step - accuracy: 0.4922 - auc: 0.8463 - loss: 0.2478 - precision: 0.6061 - recall: 0.3573 - val_accuracy: 0.7702 - val_auc: 0.9547 - val_loss: 0.1742 - val_precision: 0.8987 - val_recall: 0.6043 - learning_rate: 5.0000e-05\n",
            "Epoch 4/30\n",
            "\u001b[1m650/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m3:03\u001b[0m 913ms/step - accuracy: 0.6409 - auc: 0.9225 - loss: 0.2009 - precision: 0.7315 - recall: 0.5224"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ConvNeXtTiny\n",
        "from tensorflow.keras.applications.convnext import preprocess_input\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.losses import CategoricalFocalCrossentropy\n",
        "\n",
        "# PATHS\n",
        "TRAIN_DIR = \"/content/final_data/train\"\n",
        "VAL_DIR = \"/content/final_data/valid/sorted\"\n",
        "TEST_DIR = \"/content/final_data/test/sorted\"\n",
        "LOCAL_CHECKPOINT_PATH = \"/content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints\"\n",
        "FINAL_MODEL_PATH = \"/content/drive/MyDrive/skin_cancer/models/convnext_tiny_full_model.h5\"\n",
        "os.makedirs(LOCAL_CHECKPOINT_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.dirname(FINAL_MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# PARAMETERS\n",
        "IMG_SIZE = (224, 224)   # ConvNeXt default input size\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "\n",
        "# CENTER CROP FUNCTION\n",
        "def center_crop_and_preprocess(img):\n",
        "    \"\"\"Crop to center square, resize, then preprocess for ConvNeXt.\"\"\"\n",
        "    h, w, _ = img.shape\n",
        "    min_side = min(h, w)\n",
        "    top = (h - min_side) // 2\n",
        "    left = (w - min_side) // 2\n",
        "    img = img[top:top + min_side, left:left + min_side]\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# DATA AUGMENTATION\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=center_crop_and_preprocess,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode=\"nearest\",\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True\n",
        ")\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    TEST_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "\n",
        "# CLASS WEIGHTS → for Focal Loss alpha\n",
        "labels = train_gen.classes\n",
        "\n",
        "class_counts = np.bincount(labels)\n",
        "print(\"Counts per class:\", class_counts)\n",
        "\n",
        "# inverse-sqrt weighting\n",
        "alpha_array = 1.0 / np.sqrt(class_counts)\n",
        "alpha_array = alpha_array / np.sum(alpha_array)\n",
        "print(\"New alpha_array:\", alpha_array)\n",
        "\n",
        "# MODEL (ConvNeXt-Tiny)\n",
        "base_model = ConvNeXtTiny(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "# Freeze base model (warmup phase)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "output = layers.Dense(train_gen.num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# LOSS FUNCTION\n",
        "loss_fn = CategoricalFocalCrossentropy(\n",
        "    gamma=1.8,\n",
        "    alpha=alpha_array,\n",
        "    label_smoothing=0.05,\n",
        "    from_logits=False\n",
        ")\n",
        "\n",
        "# COMPILE (Warmup Phase)\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\"), AUC(name=\"auc\")]\n",
        ")\n",
        "\n",
        "# CALLBACKS\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(LOCAL_CHECKPOINT_PATH, \"ckpt-{epoch:02d}.keras\"),\n",
        "    save_weights_only=False,\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# PHASE 1: Warmup (Top Layers)\n",
        "history_warmup = model.fit(\n",
        "    train_gen,\n",
        "    epochs=2,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[checkpoint_callback, early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# PHASE 2: Fine-tuning deeper layers\n",
        "fine_tune_at = len(base_model.layers) // 2\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[fine_tune_at:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\"), AUC(name=\"auc\")]\n",
        ")\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=2,\n",
        "    callbacks=[checkpoint_callback, early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# SAVE FINAL MODEL\n",
        "model.save(FINAL_MODEL_PATH, save_format=\"tf\")\n",
        "print(f\"✅ Training complete! Full model saved at {FINAL_MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "weEZbbCJ3BsT",
        "outputId": "08e52787-2f3b-4e44-ce14-df13489e0559"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 27205 images belonging to 7 classes.\n",
            "Found 235 images belonging to 7 classes.\n",
            "Found 1470 images belonging to 7 classes.\n",
            "Counts per class: [3000 3500 4000 3000 4000 6705 3000]\n",
            "New alpha_array: [0.1609839  0.14676225 0.13546253 0.1609839  0.13546253 0.09936099\n",
            " 0.1609839 ]\n",
            "Adjusted alpha_array: [0.19660522 0.16643409 0.11816906 0.1404323  0.17725359 0.06067343\n",
            " 0.1404323 ]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/30\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940ms/step - accuracy: 0.9275 - auc: 0.9943 - loss: 0.0954 - precision: 0.9526 - recall: 0.8756\n",
            "Epoch 19: val_loss improved from inf to 0.12337, saving model to /content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints/ckpt-19.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 962ms/step - accuracy: 0.9275 - auc: 0.9943 - loss: 0.0954 - precision: 0.9526 - recall: 0.8756 - val_accuracy: 0.7702 - val_auc: 0.9568 - val_loss: 0.1234 - val_precision: 0.8104 - val_recall: 0.7277 - learning_rate: 2.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889ms/step - accuracy: 0.9288 - auc: 0.9946 - loss: 0.0922 - precision: 0.9520 - recall: 0.8776\n",
            "Epoch 20: val_loss did not improve from 0.12337\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 892ms/step - accuracy: 0.9288 - auc: 0.9946 - loss: 0.0922 - precision: 0.9520 - recall: 0.8776 - val_accuracy: 0.7064 - val_auc: 0.9355 - val_loss: 0.1353 - val_precision: 0.7729 - val_recall: 0.6809 - learning_rate: 2.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888ms/step - accuracy: 0.9367 - auc: 0.9952 - loss: 0.0886 - precision: 0.9574 - recall: 0.8891\n",
            "Epoch 21: val_loss improved from 0.12337 to 0.11890, saving model to /content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints/ckpt-21.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 894ms/step - accuracy: 0.9367 - auc: 0.9952 - loss: 0.0886 - precision: 0.9574 - recall: 0.8891 - val_accuracy: 0.8000 - val_auc: 0.9601 - val_loss: 0.1189 - val_precision: 0.8271 - val_recall: 0.7532 - learning_rate: 2.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883ms/step - accuracy: 0.9423 - auc: 0.9957 - loss: 0.0857 - precision: 0.9613 - recall: 0.8921\n",
            "Epoch 22: val_loss did not improve from 0.11890\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m754s\u001b[0m 886ms/step - accuracy: 0.9423 - auc: 0.9957 - loss: 0.0857 - precision: 0.9613 - recall: 0.8921 - val_accuracy: 0.7787 - val_auc: 0.9556 - val_loss: 0.1211 - val_precision: 0.8173 - val_recall: 0.7234 - learning_rate: 2.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883ms/step - accuracy: 0.9433 - auc: 0.9962 - loss: 0.0827 - precision: 0.9658 - recall: 0.8960\n",
            "Epoch 23: val_loss did not improve from 0.11890\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m754s\u001b[0m 886ms/step - accuracy: 0.9433 - auc: 0.9962 - loss: 0.0827 - precision: 0.9658 - recall: 0.8960 - val_accuracy: 0.7617 - val_auc: 0.9489 - val_loss: 0.1206 - val_precision: 0.7962 - val_recall: 0.7149 - learning_rate: 2.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m290/851\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:15\u001b[0m 883ms/step - accuracy: 0.9456 - auc: 0.9964 - loss: 0.0802 - precision: 0.9684 - recall: 0.9047"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3102637470.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m history_finetune = model.fit(\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ConvNeXtTiny\n",
        "from tensorflow.keras.applications.convnext import preprocess_input\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.losses import CategoricalFocalCrossentropy\n",
        "\n",
        "# PATHS\n",
        "TRAIN_DIR = \"/content/final_data/train\"\n",
        "VAL_DIR = \"/content/final_data/valid/sorted\"\n",
        "TEST_DIR = \"/content/final_data/test/sorted\"\n",
        "LOCAL_CHECKPOINT_PATH = \"/content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints\"\n",
        "FINAL_MODEL_PATH = \"/content/drive/MyDrive/skin_cancer/models/convnext_tiny_full_model.h5\"\n",
        "os.makedirs(LOCAL_CHECKPOINT_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.dirname(FINAL_MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# PARAMETERS\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "\n",
        "# CENTER CROP FUNCTION\n",
        "def center_crop_and_preprocess(img):\n",
        "    \"\"\"Crop to center square, resize, then preprocess for ConvNeXt.\"\"\"\n",
        "    h, w, _ = img.shape\n",
        "    min_side = min(h, w)\n",
        "    top = (h - min_side) // 2\n",
        "    left = (w - min_side) // 2\n",
        "    img = img[top:top + min_side, left:left + min_side]\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "# DATA AUGMENTATION\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=center_crop_and_preprocess,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode=\"nearest\",\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True\n",
        ")\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    TEST_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "\n",
        "# CLASS WEIGHTS → for Focal Loss alpha\n",
        "labels = train_gen.classes\n",
        "\n",
        "class_counts = np.bincount(labels)\n",
        "print(\"Counts per class:\", class_counts)\n",
        "\n",
        "# inverse-sqrt weighting\n",
        "alpha_array = 1.0 / np.power(class_counts, 0.6)\n",
        "alpha_array = alpha_array / np.sum(alpha_array)\n",
        "print(\"New alpha_array:\", alpha_array)\n",
        "\n",
        "# boost specific classes manually\n",
        "boost = {\n",
        "    0: 1.4,\n",
        "    1: 1.3,\n",
        "    4: 1.5,   # MEL index (increase focus)\n",
        "    5: 0.7,   # NV index (slightly reduce)\n",
        "}\n",
        "\n",
        "for idx, factor in boost.items():\n",
        "    alpha_array[idx] *= factor\n",
        "\n",
        "# re-normalize after boosting\n",
        "alpha_array = alpha_array / np.sum(alpha_array)\n",
        "\n",
        "print(\"Adjusted alpha_array:\", alpha_array)\n",
        "\n",
        "# LOSS FUNCTION\n",
        "loss_fn = CategoricalFocalCrossentropy(\n",
        "    gamma=1.8,\n",
        "    alpha=alpha_array,\n",
        "    label_smoothing=0.05,\n",
        "    from_logits=False\n",
        ")\n",
        "\n",
        "# CALLBACKS\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(LOCAL_CHECKPOINT_PATH, \"ckpt-{epoch:02d}.keras\"),\n",
        "    save_weights_only=False,\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints/ckpt-18.keras', compile=False)\n",
        "# Re-compile with smaller LR\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\"), AUC(name=\"auc\")]\n",
        ")\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=18,\n",
        "    callbacks=[checkpoint_callback, early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# SAVE FINAL MODEL\n",
        "model.save(FINAL_MODEL_PATH, save_format=\"tf\")\n",
        "print(f\"✅ Training complete! Full model saved at {FINAL_MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbVPnrOBmmZC",
        "outputId": "33a9d5dc-fa4c-43ee-877c-f777fa7307c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1470 images belonging to 7 classes.\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 465ms/step\n",
            "\n",
            "===== Classification Report =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       AKIEC     0.6364    0.4667    0.5385        30\n",
            "         BCC     0.7821    0.6559    0.7135        93\n",
            "         BKL     0.6000    0.8295    0.6963       217\n",
            "          DF     0.6786    0.7600    0.7170        25\n",
            "         MEL     0.5312    0.4971    0.5136       171\n",
            "          NV     0.9214    0.8636    0.8915       909\n",
            "        VASC     0.7333    0.8800    0.8000        25\n",
            "\n",
            "    accuracy                         0.7932      1470\n",
            "   macro avg     0.6976    0.7075    0.6958      1470\n",
            "weighted avg     0.8066    0.7932    0.7958      1470\n",
            "\n",
            "Macro F1: 0.6958\n",
            "\n",
            "===== Confusion Matrix =====\n",
            "[[ 14   0  14   0   2   0   0]\n",
            " [  2  61  18   2   3   5   2]\n",
            " [  2   5 180   3  13  11   3]\n",
            " [  0   1   2  19   0   3   0]\n",
            " [  2   3  32   0  85  47   2]\n",
            " [  2   7  54   3  57 785   1]\n",
            " [  0   1   0   1   0   1  22]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.convnext import preprocess_input\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "# Paths\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/skin_cancer/models/convnext_tiny_checkpoints/ckpt-21.keras\"\n",
        "TEST_DIR = \"/content/final_data/test/sorted\"\n",
        "IMG_SIZE = (224, 224)  # Use same as training\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Preprocessing and Generators\n",
        "def center_crop_and_preprocess(img):\n",
        "    \"\"\"Crop to center square, resize, then preprocess for ConvNeXt.\"\"\"\n",
        "    h, w, _ = img.shape\n",
        "    min_side = min(h, w)\n",
        "    top = (h - min_side) // 2\n",
        "    left = (w - min_side) // 2\n",
        "    img = img[top:top + min_side, left:left + min_side]\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Load Model & Evaluate\n",
        "model = tf.keras.models.load_model(CHECKPOINT_PATH, compile=False)\n",
        "\n",
        "steps = int(np.ceil(test_gen.samples / test_gen.batch_size))\n",
        "preds = model.predict(test_gen, steps=steps, verbose=1)\n",
        "\n",
        "# Evaluation Metrics\n",
        "y_true = test_gen.classes\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "target_names = list(test_gen.class_indices.keys())\n",
        "\n",
        "print(\"\\n===== Classification Report =====\")\n",
        "print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n",
        "\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(\"Macro F1:\", round(macro_f1, 4))\n",
        "\n",
        "print(\"\\n===== Confusion Matrix =====\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
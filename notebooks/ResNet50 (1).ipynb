{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjy-6XRdUBqE",
        "outputId": "b379a29e-b58c-4d39-f14e-9db7cbc1783f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yuPM2jHUHCf"
      },
      "outputs": [],
      "source": [
        "#!tar -cf /content/drive/MyDrive/skin_cancer/final_data.tar -C /content/drive/MyDrive/skin_cancer final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4iRWky2UG_B"
      },
      "outputs": [],
      "source": [
        "!tar -xf /content/drive/MyDrive/skin_cancer/final_data.tar -C /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.losses import CategoricalFocalCrossentropy\n",
        "\n",
        "# PATHS\n",
        "TRAIN_DIR = \"/content/final_data/train\"\n",
        "VAL_DIR = \"/content/final_data/valid/sorted\"\n",
        "TEST_DIR = \"/content/final_data/test/sorted\"\n",
        "LOCAL_CHECKPOINT_PATH = \"/content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints\"\n",
        "FINAL_MODEL_PATH = \"/content/drive/MyDrive/skin_cancer/models/resnet50_new_full_model.h5\"\n",
        "os.makedirs(LOCAL_CHECKPOINT_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.dirname(FINAL_MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# PARAMETERS\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "# CENTER CROP FUNCTION\n",
        "def center_crop_and_preprocess(img):\n",
        "    \"\"\"Crops the image to a square center and resizes to 224x224, then applies preprocess_input.\"\"\"\n",
        "    h, w, _ = img.shape\n",
        "    min_side = min(h, w)\n",
        "    top = (h - min_side) // 2\n",
        "    left = (w - min_side) // 2\n",
        "    img = img[top:top + min_side, left:left + min_side]\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# DATA AUGMENTATION\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=center_crop_and_preprocess,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True\n",
        ")\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    TEST_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "\n",
        "# CLASS WEIGHTS → for Focal Loss alpha\n",
        "labels = train_gen.classes\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "class_weights_dict = dict(zip(np.unique(labels), class_weights_array))\n",
        "print(\"Class weights:\", class_weights_dict)\n",
        "\n",
        "# Normalize α for focal loss\n",
        "alpha_array = np.array([class_weights_dict[i] for i in sorted(class_weights_dict.keys())], dtype=np.float32)\n",
        "alpha_array = alpha_array / np.sum(alpha_array)\n",
        "print(\"Alpha array:\", alpha_array)\n",
        "\n",
        "# MODEL (ResNet50)\n",
        "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base model first (warmup)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "output = layers.Dense(train_gen.num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# LOSS FUNCTION\n",
        "loss_fn = CategoricalFocalCrossentropy(\n",
        "    gamma=2,\n",
        "    alpha=alpha_array,\n",
        "    label_smoothing=0.05,\n",
        "    from_logits=False\n",
        ")\n",
        "\n",
        "# COMPILE (Warmup Phase)\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\"), AUC(name=\"auc\")]\n",
        ")\n",
        "\n",
        "# CALLBACKS\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(LOCAL_CHECKPOINT_PATH, \"ckpt-{epoch:02d}.keras\"),\n",
        "    save_weights_only=False,\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# PHASE 1: Warmup (Top Layers)\n",
        "history_warmup = model.fit(\n",
        "    train_gen,\n",
        "    epochs=1,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[checkpoint_callback, early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# PHASE 2: Fine-tune deeper layers\n",
        "set_trainable = False\n",
        "for layer in base_model.layers:\n",
        "    if \"conv3\" in layer.name or \"conv4\" in layer.name or \"conv5\" in layer.name:\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\"), AUC(name=\"auc\")]\n",
        ")\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=1,\n",
        "    callbacks=[checkpoint_callback, early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# SAVE FINAL MODEL\n",
        "model.save(FINAL_MODEL_PATH, save_format=\"tf\")\n",
        "print(f\"✅ Training complete! Full model saved at {FINAL_MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1QJ_4CghCEsi",
        "outputId": "93f236cd-ee90-43ec-f1ba-706042183043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 27205 images belonging to 7 classes.\n",
            "Found 235 images belonging to 7 classes.\n",
            "Found 1470 images belonging to 7 classes.\n",
            "Class weights: {np.int32(0): np.float64(1.2954761904761904), np.int32(1): np.float64(1.1104081632653062), np.int32(2): np.float64(0.9716071428571429), np.int32(3): np.float64(1.2954761904761904), np.int32(4): np.float64(0.9716071428571429), np.int32(5): np.float64(0.5796314051347609), np.int32(6): np.float64(1.2954761904761904)}\n",
            "Alpha array: [0.17227803 0.14766689 0.12920852 0.17227803 0.12920852 0.07708189\n",
            " 0.17227803]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646ms/step - accuracy: 0.3612 - auc: 0.7369 - loss: 0.3435 - precision: 0.4201 - recall: 0.2675\n",
            "Epoch 1: val_loss improved from inf to 0.23253, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-01.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 672ms/step - accuracy: 0.3612 - auc: 0.7369 - loss: 0.3435 - precision: 0.4202 - recall: 0.2675 - val_accuracy: 0.6511 - val_auc: 0.9079 - val_loss: 0.2325 - val_precision: 0.7605 - val_recall: 0.5404 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767ms/step - accuracy: 0.5381 - auc: 0.8695 - loss: 0.2527 - precision: 0.6200 - recall: 0.4356\n",
            "Epoch 2: val_loss improved from 0.23253 to 0.18856, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-02.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m730s\u001b[0m 794ms/step - accuracy: 0.5381 - auc: 0.8696 - loss: 0.2527 - precision: 0.6201 - recall: 0.4356 - val_accuracy: 0.8043 - val_auc: 0.9662 - val_loss: 0.1886 - val_precision: 0.9067 - val_recall: 0.7447 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743ms/step - accuracy: 0.6888 - auc: 0.9401 - loss: 0.1910 - precision: 0.7869 - recall: 0.5657\n",
            "Epoch 3: val_loss did not improve from 0.18856\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m635s\u001b[0m 746ms/step - accuracy: 0.6888 - auc: 0.9401 - loss: 0.1910 - precision: 0.7869 - recall: 0.5658 - val_accuracy: 0.6979 - val_auc: 0.9381 - val_loss: 0.1886 - val_precision: 0.8204 - val_recall: 0.5830 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737ms/step - accuracy: 0.7365 - auc: 0.9556 - loss: 0.1711 - precision: 0.8304 - recall: 0.6030\n",
            "Epoch 4: val_loss improved from 0.18856 to 0.17937, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-04.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 753ms/step - accuracy: 0.7366 - auc: 0.9556 - loss: 0.1710 - precision: 0.8304 - recall: 0.6030 - val_accuracy: 0.7745 - val_auc: 0.9656 - val_loss: 0.1794 - val_precision: 0.8967 - val_recall: 0.7021 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744ms/step - accuracy: 0.7838 - auc: 0.9680 - loss: 0.1535 - precision: 0.8657 - recall: 0.6517\n",
            "Epoch 5: val_loss improved from 0.17937 to 0.16161, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-05.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 759ms/step - accuracy: 0.7838 - auc: 0.9680 - loss: 0.1535 - precision: 0.8658 - recall: 0.6517 - val_accuracy: 0.7489 - val_auc: 0.9448 - val_loss: 0.1616 - val_precision: 0.8621 - val_recall: 0.6383 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738ms/step - accuracy: 0.7986 - auc: 0.9729 - loss: 0.1402 - precision: 0.8786 - recall: 0.6668\n",
            "Epoch 6: val_loss did not improve from 0.16161\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 742ms/step - accuracy: 0.7987 - auc: 0.9729 - loss: 0.1402 - precision: 0.8786 - recall: 0.6668 - val_accuracy: 0.7574 - val_auc: 0.9617 - val_loss: 0.1655 - val_precision: 0.8771 - val_recall: 0.6681 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723ms/step - accuracy: 0.8224 - auc: 0.9774 - loss: 0.1261 - precision: 0.8932 - recall: 0.6996\n",
            "Epoch 7: val_loss improved from 0.16161 to 0.14652, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-07.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 739ms/step - accuracy: 0.8224 - auc: 0.9774 - loss: 0.1261 - precision: 0.8932 - recall: 0.6996 - val_accuracy: 0.7830 - val_auc: 0.9683 - val_loss: 0.1465 - val_precision: 0.8913 - val_recall: 0.6979 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736ms/step - accuracy: 0.8319 - auc: 0.9796 - loss: 0.1138 - precision: 0.9008 - recall: 0.7133\n",
            "Epoch 8: val_loss improved from 0.14652 to 0.13009, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-08.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 742ms/step - accuracy: 0.8319 - auc: 0.9796 - loss: 0.1138 - precision: 0.9008 - recall: 0.7133 - val_accuracy: 0.7702 - val_auc: 0.9550 - val_loss: 0.1301 - val_precision: 0.8564 - val_recall: 0.6851 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732ms/step - accuracy: 0.8390 - auc: 0.9810 - loss: 0.1021 - precision: 0.9038 - recall: 0.7309\n",
            "Epoch 9: val_loss improved from 0.13009 to 0.11685, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-09.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 747ms/step - accuracy: 0.8390 - auc: 0.9810 - loss: 0.1021 - precision: 0.9038 - recall: 0.7309 - val_accuracy: 0.7532 - val_auc: 0.9540 - val_loss: 0.1168 - val_precision: 0.8307 - val_recall: 0.6681 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741ms/step - accuracy: 0.8561 - auc: 0.9842 - loss: 0.0899 - precision: 0.9153 - recall: 0.7526\n",
            "Epoch 10: val_loss did not improve from 0.11685\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 744ms/step - accuracy: 0.8561 - auc: 0.9842 - loss: 0.0899 - precision: 0.9153 - recall: 0.7526 - val_accuracy: 0.7702 - val_auc: 0.9502 - val_loss: 0.1222 - val_precision: 0.8478 - val_recall: 0.6638 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725ms/step - accuracy: 0.8699 - auc: 0.9869 - loss: 0.0790 - precision: 0.9222 - recall: 0.7704\n",
            "Epoch 11: val_loss improved from 0.11685 to 0.10757, saving model to /content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-11.keras\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 739ms/step - accuracy: 0.8699 - auc: 0.9869 - loss: 0.0790 - precision: 0.9222 - recall: 0.7704 - val_accuracy: 0.7660 - val_auc: 0.9544 - val_loss: 0.1076 - val_precision: 0.8641 - val_recall: 0.6766 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729ms/step - accuracy: 0.8754 - auc: 0.9882 - loss: 0.0713 - precision: 0.9277 - recall: 0.7853\n",
            "Epoch 12: val_loss did not improve from 0.10757\n",
            "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 732ms/step - accuracy: 0.8754 - auc: 0.9882 - loss: 0.0713 - precision: 0.9277 - recall: 0.7853 - val_accuracy: 0.7191 - val_auc: 0.9498 - val_loss: 0.1141 - val_precision: 0.8162 - val_recall: 0.6426 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m439/851\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4:57\u001b[0m 721ms/step - accuracy: 0.8743 - auc: 0.9880 - loss: 0.0665 - precision: 0.9267 - recall: 0.7842"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1383888028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m history_finetune = model.fit(\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "# Paths\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/skin_cancer/models/resnet50_new_checkpoints/ckpt-11.keras\"\n",
        "TEST_DIR = \"/content/final_data/test/sorted\"\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Preprocessing and Generators\n",
        "def center_crop_and_preprocess(img):\n",
        "    h, w, _ = img.shape\n",
        "    min_side = min(h, w)\n",
        "    top = (h - min_side) // 2\n",
        "    left = (w - min_side) // 2\n",
        "    img = img[top:top + min_side, left:left + min_side]\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=center_crop_and_preprocess)\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Load Model & Evaluate\n",
        "model = tf.keras.models.load_model(CHECKPOINT_PATH, compile=False)\n",
        "\n",
        "steps = int(np.ceil(test_gen.samples / test_gen.batch_size))\n",
        "preds = model.predict(test_gen, steps=steps, verbose=1)\n",
        "\n",
        "y_true = test_gen.classes\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "target_names = list(test_gen.class_indices.keys())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n",
        "print(\"Macro F1:\", f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgIN0K8bOGVJ",
        "outputId": "59aeeb33-d919-4fb6-b36c-5f156c09985e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1470 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 455ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       AKIEC     0.4857    0.5667    0.5231        30\n",
            "         BCC     0.7468    0.6344    0.6860        93\n",
            "         BKL     0.6884    0.6313    0.6587       217\n",
            "          DF     0.5000    0.7200    0.5902        25\n",
            "         MEL     0.4505    0.7719    0.5690       171\n",
            "          NV     0.9264    0.8174    0.8685       909\n",
            "        VASC     0.8462    0.8800    0.8627        25\n",
            "\n",
            "    accuracy                         0.7673      1470\n",
            "   macro avg     0.6634    0.7174    0.6797      1470\n",
            "weighted avg     0.8070    0.7673    0.7793      1470\n",
            "\n",
            "Macro F1: 0.6797356835683035\n",
            "Confusion Matrix:\n",
            " [[ 17   4   3   3   2   1   0]\n",
            " [  5  59   6   0  16   7   0]\n",
            " [  9   8 137   0  43  20   0]\n",
            " [  1   0   1  18   2   2   1]\n",
            " [  1   0   8   3 132  27   0]\n",
            " [  2   8  44  11  98 743   3]\n",
            " [  0   0   0   1   0   2  22]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}